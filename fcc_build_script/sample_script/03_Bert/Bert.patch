diff --git a/official/nlp/bert/bert_models.py b/official/nlp/bert/bert_models.py
index 8a0fceb72..d8a2fc086 100644
--- a/official/nlp/bert/bert_models.py
+++ b/official/nlp/bert/bert_models.py
@@ -35,6 +35,13 @@ class BertPretrainLossAndMetricLayer(tf.keras.layers.Layer):
         'vocab_size': vocab_size,
     }
 
+  def get_config(self):
+    config = super().get_config()
+    config.update({
+        'vocab_size': vocab_size,
+    })
+    return config
+
   def _add_metrics(self, lm_output, lm_labels, lm_label_weights,
                    lm_example_loss, sentence_output, sentence_labels,
                    next_sentence_loss):
diff --git a/official/nlp/bert/input_pipeline.py b/official/nlp/bert/input_pipeline.py
index 0c0f7615c..69c4f64c5 100644
--- a/official/nlp/bert/input_pipeline.py
+++ b/official/nlp/bert/input_pipeline.py
@@ -15,7 +15,7 @@
 """BERT model input pipelines."""
 
 import tensorflow as tf
-
+from absl import logging
 
 def decode_record(record, name_to_features):
   """Decodes a record to a TensorFlow example."""
@@ -58,6 +58,7 @@ def create_pretrain_dataset(input_patterns,
                             seq_length,
                             max_predictions_per_seq,
                             batch_size,
+                            hvd=None,
                             is_training=True,
                             input_pipeline_context=None,
                             use_next_sentence_label=True,
@@ -88,11 +89,16 @@ def create_pretrain_dataset(input_patterns,
     if not tf.io.gfile.glob(input_pattern):
       raise ValueError('%s does not match any files.' % input_pattern)
 
-  dataset = tf.data.Dataset.list_files(input_patterns, shuffle=is_training)
+  dataset = tf.data.Dataset.list_files(input_patterns, shuffle=is_training, seed=0)
 
-  if input_pipeline_context and input_pipeline_context.num_input_pipelines > 1:
+  if input_pipeline_context and input_pipeline_context.num_input_pipelines > 1 and not hvd:
     dataset = dataset.shard(input_pipeline_context.num_input_pipelines,
                             input_pipeline_context.input_pipeline_id)
+  if hvd:
+    if len(list(dataset.as_numpy_iterator())) < hvd.size():
+      logging.error("The number of processes is greater than the number of data files.") 
+      exit()
+    dataset = dataset.shard(hvd.size(), hvd.rank())
   if is_training:
     dataset = dataset.repeat()
 
@@ -151,6 +157,7 @@ def create_pretrain_dataset(input_patterns,
 def create_classifier_dataset(file_path,
                               seq_length,
                               batch_size,
+                              hvd=None,
                               is_training=True,
                               input_pipeline_context=None,
                               label_type=tf.int64,
@@ -170,9 +177,11 @@ def create_classifier_dataset(file_path,
 
   # The dataset is always sharded by number of hosts.
   # num_input_pipelines is the number of hosts rather than number of cores.
-  if input_pipeline_context and input_pipeline_context.num_input_pipelines > 1:
+  if input_pipeline_context and input_pipeline_context.num_input_pipelines > 1 and not hvd:
     dataset = dataset.shard(input_pipeline_context.num_input_pipelines,
                             input_pipeline_context.input_pipeline_id)
+  if is_training and hvd:
+    dataset = dataset.shard(hvd.size(), hvd.rank())
 
   def _select_data_from_record(record):
     x = {
diff --git a/official/nlp/bert/run_classifier.py b/official/nlp/bert/run_classifier.py
index 1979af5fb..6bf2f74a1 100644
--- a/official/nlp/bert/run_classifier.py
+++ b/official/nlp/bert/run_classifier.py
@@ -34,6 +34,7 @@ from official.nlp.bert import configs as bert_configs
 from official.nlp.bert import input_pipeline
 from official.nlp.bert import model_saving_utils
 from official.utils.misc import keras_utils
+import horovod.tensorflow.keras as hvd
 
 flags.DEFINE_enum(
     'mode', 'train_and_eval', ['train_and_eval', 'export_only', 'predict'],
@@ -63,6 +64,7 @@ flags.DEFINE_integer(
     'evaluated every N/n samples.')
 flags.DEFINE_integer('train_batch_size', 32, 'Batch size for training.')
 flags.DEFINE_integer('eval_batch_size', 32, 'Batch size for evaluation.')
+flags.DEFINE_bool('use_horovod', False, 'train with horovod')
 
 common_flags.define_common_bert_flags()
 
@@ -100,15 +102,28 @@ def get_dataset_fn(input_file_pattern,
     """Returns tf.data.Dataset for distributed BERT pretraining."""
     batch_size = ctx.get_per_replica_batch_size(
         global_batch_size) if ctx else global_batch_size
-    dataset = input_pipeline.create_classifier_dataset(
-        tf.io.gfile.glob(input_file_pattern),
-        max_seq_length,
-        batch_size,
-        is_training=is_training,
-        input_pipeline_context=ctx,
-        label_type=label_type,
-        include_sample_weights=include_sample_weights,
-        num_samples=num_samples)
+    if use_hvd and FLAGS.use_keras_compile_fit:         # xdote
+      dataset = input_pipeline.create_classifier_dataset(
+          tf.io.gfile.glob(input_file_pattern),
+          max_seq_length,
+          batch_size,
+          hvd=hvd,
+          is_training=is_training,
+          input_pipeline_context=ctx,
+          label_type=label_type,
+          include_sample_weights=include_sample_weights,
+          num_samples=num_samples)
+    else:
+      dataset = input_pipeline.create_classifier_dataset(
+          tf.io.gfile.glob(input_file_pattern),
+          max_seq_length,
+          batch_size,
+          hvd=hvd,
+          is_training=is_training,
+          input_pipeline_context=ctx,
+          label_type=label_type,
+          include_sample_weights=include_sample_weights,
+          num_samples=num_samples)
     return dataset
 
   return _dataset_fn
@@ -127,6 +142,7 @@ def run_bert_classifier(strategy,
                         init_checkpoint,
                         train_input_fn,
                         eval_input_fn,
+                        use_hvd=False,
                         training_callbacks=True,
                         custom_callbacks=None,
                         custom_metrics=None):
@@ -144,10 +160,18 @@ def run_bert_classifier(strategy,
             max_seq_length,
             hub_module_url=FLAGS.hub_module_url,
             hub_module_trainable=FLAGS.hub_module_trainable))
-    optimizer = optimization.create_optimizer(initial_lr,
-                                              steps_per_epoch * epochs,
-                                              warmup_steps, FLAGS.end_lr,
-                                              FLAGS.optimizer_type)
+    if use_hvd and use_keras_compile_fit:
+      # Scale the learning rate by the number of workers refer to Horovod with Keras
+      # (https://horovod.readthedocs.io/en/stable/keras.html)
+      optimizer = optimization.create_optimizer(initial_lr * hvd.size(),
+                                                steps_per_epoch * epochs // hvd.size(),
+                                                warmup_steps, FLAGS.end_lr,
+                                                FLAGS.optimizer_type)
+    else:
+      optimizer = optimization.create_optimizer(initial_lr,
+                                                steps_per_epoch * epochs,
+                                                warmup_steps, FLAGS.end_lr,
+                                                FLAGS.optimizer_type)
     classifier_model.optimizer = performance.configure_optimizer(
         optimizer,
         use_float16=common_flags.use_float16(),
@@ -192,6 +216,7 @@ def run_bert_classifier(strategy,
       steps_per_epoch,
       steps_per_loop,
       eval_steps,
+      use_hvd=hvd,
       training_callbacks=training_callbacks,
       custom_callbacks=custom_callbacks)
 
@@ -208,6 +233,7 @@ def run_keras_compile_fit(model_dir,
                           steps_per_epoch,
                           steps_per_loop,
                           eval_steps,
+                          use_hvd=False,
                           training_callbacks=True,
                           custom_callbacks=None):
   """Runs BERT classifier model using Keras compile/fit API."""
@@ -224,36 +250,77 @@ def run_keras_compile_fit(model_dir,
 
     if not isinstance(metric_fn, (list, tuple)):
       metric_fn = [metric_fn]
-    bert_model.compile(
-        optimizer=optimizer,
-        loss=loss_fn,
-        metrics=[fn() for fn in metric_fn],
-        steps_per_execution=steps_per_loop)
-
-    summary_dir = os.path.join(model_dir, 'summaries')
-    summary_callback = tf.keras.callbacks.TensorBoard(summary_dir)
-    checkpoint = tf.train.Checkpoint(model=bert_model, optimizer=optimizer)
-    checkpoint_manager = tf.train.CheckpointManager(
-        checkpoint,
-        directory=model_dir,
-        max_to_keep=None,
-        step_counter=optimizer.iterations,
-        checkpoint_interval=0)
-    checkpoint_callback = keras_utils.SimpleCheckpoint(checkpoint_manager)
-
-    if training_callbacks:
-      if custom_callbacks is not None:
-        custom_callbacks += [summary_callback, checkpoint_callback]
-      else:
-        custom_callbacks = [summary_callback, checkpoint_callback]
-
-    history = bert_model.fit(
-        x=training_dataset,
-        validation_data=evaluation_dataset,
-        steps_per_epoch=steps_per_epoch,
-        epochs=epochs,
-        validation_steps=eval_steps,
-        callbacks=custom_callbacks)
+
+    if use_hvd:
+      optimizer = hvd.DistributedOptimizer(optimizer)
+      bert_model.compile(
+          optimizer=optimizer,
+          loss=loss_fn,
+          metrics=[fn() for fn in metric_fn],
+          steps_per_execution=steps_per_loop)
+      if hvd.rank() == 0:
+        summary_dir = os.path.join(model_dir, 'summaries')
+        summary_callback = tf.keras.callbacks.TensorBoard(summary_dir)
+        checkpoint = tf.train.Checkpoint(model=bert_model, optimizer=optimizer)
+        checkpoint_manager = tf.train.CheckpointManager(
+            checkpoint,
+            directory=model_dir,
+            max_to_keep=None,
+            step_counter=optimizer.iterations,
+            checkpoint_interval=0)
+        checkpoint_callback = keras_utils.SimpleCheckpoint(checkpoint_manager)
+
+      steps_per_epoch = steps_per_epoch // hvd.size()
+
+      if training_callbacks:
+        if custom_callbacks is not None:
+          #custom_callbacks += [summary_callback, checkpoint_callback]
+          custom_callbacks += [hvd.callbacks.BroadcastGlobalVariablesCallback(0),]
+        else:
+          custom_callbacks = [hvd.callbacks.BroadcastGlobalVariablesCallback(0),]
+    else:
+      bert_model.compile(
+          optimizer=optimizer,
+          loss=loss_fn,
+          metrics=[fn() for fn in metric_fn],
+          steps_per_execution=steps_per_loop)
+      summary_dir = os.path.join(model_dir, 'summaries')
+      summary_callback = tf.keras.callbacks.TensorBoard(summary_dir)
+      checkpoint = tf.train.Checkpoint(model=bert_model, optimizer=optimizer)
+      checkpoint_manager = tf.train.CheckpointManager(
+          checkpoint,
+          directory=model_dir,
+          max_to_keep=None,
+          step_counter=optimizer.iterations,
+          checkpoint_interval=0)
+      checkpoint_callback = keras_utils.SimpleCheckpoint(checkpoint_manager)
+
+      if training_callbacks:
+        if custom_callbacks is not None:
+          #custom_callbacks += [summary_callback, checkpoint_callback]
+          custom_callbacks += [hvd.callbacks.BroadcastGlobalVariablesCallback(0),]
+        else:
+          custom_callbacks = [hvd.callbacks.BroadcastGlobalVariablesCallback(0),]
+        if hvd.rank() == 0:
+
+    if use_hvd:
+      history = bert_model.fit(
+          x=training_dataset,
+          validation_data=evaluation_dataset,
+          steps_per_epoch=steps_per_epoch,
+          epochs=epochs,
+          validation_steps=eval_steps,
+          callbacks=custom_callbacks,
+          verbose=2 if hvd.rank() == 0 else 0)
+    else:
+      history = bert_model.fit(
+          x=training_dataset,
+          validation_data=evaluation_dataset,
+          steps_per_epoch=steps_per_epoch,
+          epochs=epochs,
+          validation_steps=eval_steps,
+          callbacks=custom_callbacks)
+
     stats = {'total_training_steps': steps_per_epoch * epochs}
     if 'loss' in history.history:
       stats['train_loss'] = history.history['loss'][-1]
@@ -362,6 +429,7 @@ def export_classifier(model_export_path, input_meta_data, bert_config,
 def run_bert(strategy,
              input_meta_data,
              model_config,
+             use_hvd=False,
              train_input_fn=None,
              eval_input_fn=None,
              init_checkpoint=None,
@@ -410,12 +478,18 @@ def run_bert(strategy,
       init_checkpoint or FLAGS.init_checkpoint,
       train_input_fn,
       eval_input_fn,
+      use_hvd=use_hvd,
       custom_callbacks=custom_callbacks,
       custom_metrics=custom_metrics)
 
   if FLAGS.model_export_path:
-    model_saving_utils.export_bert_model(
-        FLAGS.model_export_path, model=trained_model)
+    if use_hvd and FLAGS.use_keras_compile_fit:
+      if hvd.rank() == 0:
+        model_saving_utils.export_bert_model(
+            FLAGS.model_export_path, model=trained_model)
+    else:
+      model_saving_utils.export_bert_model(
+          FLAGS.model_export_path, model=trained_model)
   return trained_model
 
 
@@ -433,6 +507,13 @@ def custom_main(custom_callbacks=None, custom_metrics=None):
   label_type = LABEL_TYPES_MAP[input_meta_data.get('label_type', 'int')]
   include_sample_weights = input_meta_data.get('has_sample_weights', False)
 
+  use_hvd = FLAGS.use_horovod
+  if use_hvd and not FLAGS.use_keras_compile_fit:
+    logging.error("Please make sure 'use_keras_compile_fit' is True if you want to run `BERT` with horovod.")
+    exit()
+  if use_hvd and FLAGS.use_keras_compile_fit:
+    hvd.init()
+
   if not FLAGS.model_dir:
     FLAGS.model_dir = '/tmp/bert20/'
 
@@ -492,6 +573,7 @@ def custom_main(custom_callbacks=None, custom_metrics=None):
       input_meta_data['max_seq_length'],
       FLAGS.train_batch_size,
       is_training=True,
+      use_hvd=use_hvd,
       label_type=label_type,
       include_sample_weights=include_sample_weights,
       num_samples=FLAGS.train_data_size)
@@ -501,6 +583,7 @@ def custom_main(custom_callbacks=None, custom_metrics=None):
       bert_config,
       train_input_fn,
       eval_input_fn,
+      use_hvd=use_hvd,
       custom_callbacks=custom_callbacks,
       custom_metrics=custom_metrics)
 
diff --git a/official/nlp/bert/run_pretraining.py b/official/nlp/bert/run_pretraining.py
index 6e21b991d..84324005e 100644
--- a/official/nlp/bert/run_pretraining.py
+++ b/official/nlp/bert/run_pretraining.py
@@ -29,6 +29,12 @@ from official.nlp.bert import configs
 from official.nlp.bert import input_pipeline
 from official.nlp.bert import model_training_utils
 
+import json
+import math
+import os
+from official.nlp.bert import model_saving_utils
+from official.utils.misc import keras_utils
+import horovod.tensorflow.keras as hvd
 
 flags.DEFINE_string('input_files', None,
                     'File path to retrieve training data for pre-training.')
@@ -50,6 +56,7 @@ flags.DEFINE_bool('use_next_sentence_label', True,
 flags.DEFINE_bool('train_summary_interval', 0, 'Step interval for training '
                   'summaries. If the value is a negative number, '
                   'then training summaries are not enabled.')
+flags.DEFINE_bool('use_horovod', False, 'train with horovod')
 
 common_flags.define_common_bert_flags()
 
@@ -57,21 +64,32 @@ FLAGS = flags.FLAGS
 
 
 def get_pretrain_dataset_fn(input_file_pattern, seq_length,
-                            max_predictions_per_seq, global_batch_size,
+                            max_predictions_per_seq, global_batch_size, use_hvd=False,
                             use_next_sentence_label=True):
   """Returns input dataset from input file string."""
   def _dataset_fn(ctx=None):
     """Returns tf.data.Dataset for distributed BERT pretraining."""
     input_patterns = input_file_pattern.split(',')
-    batch_size = ctx.get_per_replica_batch_size(global_batch_size)
-    train_dataset = input_pipeline.create_pretrain_dataset(
-        input_patterns,
-        seq_length,
-        max_predictions_per_seq,
-        batch_size,
-        is_training=True,
-        input_pipeline_context=ctx,
-        use_next_sentence_label=use_next_sentence_label)
+    batch_size = ctx.get_per_replica_batch_size(global_batch_size) if ctx else global_batch_size
+    if use_hvd:
+      train_dataset = input_pipeline.create_pretrain_dataset(
+          input_patterns,
+          seq_length,
+          max_predictions_per_seq,
+          batch_size,
+          hvd=hvd,
+          is_training=True,
+          input_pipeline_context=ctx,
+          use_next_sentence_label=use_next_sentence_label)
+    else:
+      train_dataset = input_pipeline.create_pretrain_dataset(
+          input_patterns,
+          seq_length,
+          max_predictions_per_seq,
+          batch_size,
+          is_training=True,
+          input_pipeline_context=ctx,
+          use_next_sentence_label=use_next_sentence_label)
     return train_dataset
 
   return _dataset_fn
@@ -86,7 +104,7 @@ def get_loss_fn():
   return _bert_pretrain_loss_fn
 
 
-def run_customized_training(strategy,
+def run_bert_pretrain(strategy,
                             bert_config,
                             init_checkpoint,
                             max_seq_length,
@@ -103,6 +121,8 @@ def run_customized_training(strategy,
                             train_batch_size,
                             use_next_sentence_label=True,
                             train_summary_interval=0,
+                            use_hvd=False,
+                            use_keras_compile_fit=False,
                             custom_callbacks=None,
                             explicit_allreduce=False,
                             pre_allreduce_callbacks=None,
@@ -112,7 +132,7 @@ def run_customized_training(strategy,
 
   train_input_fn = get_pretrain_dataset_fn(input_files, max_seq_length,
                                            max_predictions_per_seq,
-                                           train_batch_size,
+                                           train_batch_size, use_hvd,
                                            use_next_sentence_label)
 
   def _get_pretrain_model():
@@ -120,15 +140,41 @@ def run_customized_training(strategy,
     pretrain_model, core_model = bert_models.pretrain_model(
         bert_config, max_seq_length, max_predictions_per_seq,
         use_next_sentence_label=use_next_sentence_label)
-    optimizer = optimization.create_optimizer(
-        initial_lr, steps_per_epoch * epochs, warmup_steps,
-        end_lr, optimizer_type)
+    if use_hvd and use_keras_compile_fit:
+      # Scale the learning rate by the number of workers refer to Horovod with Keras
+      # (https://horovod.readthedocs.io/en/stable/keras.html)
+      optimizer = optimization.create_optimizer(
+          initial_lr * hvd.size(), steps_per_epoch * epochs // hvd.size(), warmup_steps,
+          end_lr, optimizer_type)
+    else:
+      optimizer = optimization.create_optimizer(
+          initial_lr, steps_per_epoch * epochs, warmup_steps,
+          end_lr, optimizer_type)
     pretrain_model.optimizer = performance.configure_optimizer(
         optimizer,
         use_float16=common_flags.use_float16(),
         use_graph_rewrite=common_flags.use_graph_rewrite())
     return pretrain_model, core_model
 
+  if use_hvd and use_keras_compile_fit:
+    logging.info('Training using TF 2.x Keras compile/fit API with '
+                 'distribution strategy.')
+    return run_keras_compile_fit(
+        model_dir,
+        strategy,
+        _get_pretrain_model,
+        train_input_fn,
+        get_loss_fn(),
+        init_checkpoint,
+        epochs,
+        steps_per_epoch,
+        use_hvd=use_hvd,
+        custom_callbacks=custom_callbacks)
+
+  # Runs customized training loop.
+  logging.info('Training using customized training loop TF 2.0 with distributed'
+               'strategy.')
+
   trained_model = model_training_utils.run_customized_training_loop(
       strategy=strategy,
       model_fn=_get_pretrain_model,
@@ -150,18 +196,89 @@ def run_customized_training(strategy,
 
   return trained_model
 
+def run_keras_compile_fit(model_dir,
+                          strategy,
+                          model_fn,
+                          train_input_fn,
+                          loss_fn,
+                          init_checkpoint,
+                          epochs,
+                          steps_per_epoch,
+                          use_hvd=False,
+                          custom_callbacks=None):
+  """Runs BERT pretraining model using Keras compile/fit API."""
+
+  with strategy.scope():
+    training_dataset = train_input_fn()
+    bert_model, sub_model = model_fn()
+    optimizer = bert_model.optimizer
+
+    if init_checkpoint:
+      checkpoint = tf.train.Checkpoint(model=sub_model)
+      checkpoint.restore(init_checkpoint).assert_existing_objects_matched()
+
+    if use_hvd:
+      optimizer = hvd.DistributedOptimizer(optimizer)
+      bert_model.compile(optimizer=optimizer, loss=loss_fn, experimental_run_tf_function=False)
+      if custom_callbacks is not None:
+        custom_callbacks += [hvd.callbacks.BroadcastGlobalVariablesCallback(0),]
+      else:
+        custom_callbacks = [hvd.callbacks.BroadcastGlobalVariablesCallback(0),]
+      if hvd.rank() == 0:
+        summary_dir = os.path.join(model_dir, 'summaries')
+        summary_callback = tf.keras.callbacks.TensorBoard(summary_dir)
+        checkpoint_path = os.path.join(model_dir, 'checkpoint')
+        checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(
+            checkpoint_path, save_weights_only=True)
+        custom_callbacks += [summary_callback, checkpoint_callback]
+      steps_per_epoch = steps_per_epoch // hvd.size()
+    else:
+      bert_model.compile(optimizer=optimizer, loss=loss_fn)
+
+      summary_dir = os.path.join(model_dir, 'summaries')
+      summary_callback = tf.keras.callbacks.TensorBoard(summary_dir)
+      checkpoint_path = os.path.join(model_dir, 'checkpoint')
+      checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(
+          checkpoint_path, save_weights_only=True)
+
+      if custom_callbacks is not None:
+        custom_callbacks += [summary_callback, checkpoint_callback]
+      else:
+        custom_callbacks = [summary_callback, checkpoint_callback]
 
-def run_bert_pretrain(strategy, custom_callbacks=None):
+    ckpt = tf.train.Checkpoint(model=sub_model)
+    ckpt_path = os.path.join(model_dir, 'pretrained_model/bert_model.ckpt')
+
+    if use_hvd:
+      bert_model.fit(
+          x=training_dataset,
+          epochs=epochs,
+          steps_per_epoch=steps_per_epoch,
+          callbacks=custom_callbacks,
+          verbose=2 if hvd.rank() == 0 else 0)
+      if hvd.rank() == 0:
+        checkpoint.save(ckpt_path)
+    else:
+      bert_model.fit(
+          x=training_dataset,
+          epochs=epochs,
+          steps_per_epoch=steps_per_epoch,
+          callbacks=custom_callbacks)
+      checkpoint.save(ckpt_path)
+
+    return bert_model
+
+
+def run_bert(strategy, custom_callbacks=None, use_hvd=False):
   """Runs BERT pre-training."""
 
+  keras_utils.set_config_v2(FLAGS.enable_xla)
+  performance.set_mixed_precision_policy(common_flags.dtype())
+
   bert_config = configs.BertConfig.from_json_file(FLAGS.bert_config_file)
   if not strategy:
     raise ValueError('Distribution strategy is not specified.')
 
-  # Runs customized training loop.
-  logging.info('Training using customized training loop TF 2.0 with distributed'
-               'strategy.')
-
   performance.set_mixed_precision_policy(common_flags.dtype())
 
   # Only when explicit_allreduce = True, post_allreduce_callbacks and
@@ -170,7 +287,7 @@ def run_bert_pretrain(strategy, custom_callbacks=None):
   # pass the allreduced grads_and_vars to apply_gradients().
   # With explicit_allreduce = True, clip_by_global_norm is moved to after
   # allreduce.
-  return run_customized_training(
+  return run_bert_pretrain(
       strategy,
       bert_config,
       FLAGS.init_checkpoint,  # Used to initialize only the BERT submodel.
@@ -188,6 +305,8 @@ def run_bert_pretrain(strategy, custom_callbacks=None):
       FLAGS.train_batch_size,
       FLAGS.use_next_sentence_label,
       FLAGS.train_summary_interval,
+      use_hvd=use_hvd,
+      use_keras_compile_fit=FLAGS.use_keras_compile_fit,
       custom_callbacks=custom_callbacks,
       explicit_allreduce=FLAGS.explicit_allreduce,
       pre_allreduce_callbacks=[
@@ -200,6 +319,20 @@ def main(_):
   gin.parse_config_files_and_bindings(FLAGS.gin_file, FLAGS.gin_param)
   if not FLAGS.model_dir:
     FLAGS.model_dir = '/tmp/bert20/'
+  use_hvd = FLAGS.use_horovod
+  if use_hvd and not FLAGS.use_keras_compile_fit:
+    logging.error("Please make sure `use_keras_compile_fit` is `True` if you want to run `BERT` with horovod.")
+    exit()
+  if use_hvd and FLAGS.use_keras_compile_fit:
+    hvd.init()
+  if FLAGS.log_steps:
+    custom_callbacks = [keras_utils.TimeHistory(
+        batch_size=FLAGS.train_batch_size,
+        log_steps=FLAGS.log_steps,
+        logdir=FLAGS.model_dir,
+    )]
+  else:
+    custom_callbacks = None
   # Configures cluster spec for multi-worker distribution strategy.
   if FLAGS.num_gpus > 0:
     _ = distribute_utils.configure_cluster(FLAGS.worker_hosts, FLAGS.task_index)
@@ -210,9 +343,13 @@ def main(_):
       tpu_address=FLAGS.tpu)
   if strategy:
     print('***** Number of cores used : ', strategy.num_replicas_in_sync)
+    # Workaround for 'Pool.__del__, exception ignored' in the end of the process
+    # See https://github.com/tensorflow/tensorflow/issues/50487
+    import atexit
+    strategy = tf.distribute.MirroredStrategy()
+    atexit.register(strategy._extended._collective_ops._pool.close) # type: ignore
 
-  run_bert_pretrain(strategy)
-
+  run_bert(strategy, custom_callbacks, use_hvd)
 
 if __name__ == '__main__':
   app.run(main)
